{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание №3 (Оцениваемое)\n",
    "\n",
    "#### Задание №1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте текст \"Литературных анекдотов\". Напишите функцию, которая будет читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл. У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = 'C:\\\\Users\\\\Нелли\\\\Documents\\\\python-for-dh\\\\Classes\\\\9-10\\\\literary_anecdotes.txt'\n",
    "write_path = 'C:\\\\Users\\\\Нелли\\\\Documents\\\\python-for-dh\\\\Classes\\\\9-10\\\\lemmatized_anecdotes.json'\n",
    "stopwords_path = 'C:\\\\Users\\\\Нелли\\\\Documents\\\\python-for-dh\\\\Classes\\\\rus_stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatization(original_text): #создаем функцию для лемматизации текста\n",
    "    m = Mystem()\n",
    "    new_text = [w.strip(punctuation) for w in original_text.split()] #очищаем текст от пунктуации\n",
    "    united_line = ' '.join(new_text) # преобразуем список в строку\n",
    "    lemma = m.lemmatize(united_line) #создаем переменную, в которую будет записан лемматизованный текст\n",
    "    return ''.join(lemma) #возвращаем лематизированный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(read_path, write_path): #создаем функцию для чтениня и записи нового результата в файл\n",
    "    f = open(read_path, 'r', encoding='utf-8') #считываем текст из файла\n",
    "    text = f.read()\n",
    "    lemmatized_text = text_lemmatization(text) #записываем в переменную результата вызова функции лемматизации\n",
    "    with open(write_path, 'w', encoding='utf-8') as f: #открываем файл\n",
    "        json.dump(lemmatized_text, f, ensure_ascii=False) #записываем в файл новый результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessing(read_path, write_path) #вызываем созданную раннее функцию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистите лемматизированный текст от стоп-слов и посчитайте ipm для оставшихся. Выведите 20 самых частотных по этому параметру слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(original_text, stopwords): #создадим функцию для удаления стоп-слов\n",
    "    new_stopwordless_text = [w.strip(punctuation) for w in original_text if w not in stopwords] #если слово не входит в список в стоп-слов, оставляем его и добавляем в массив\n",
    "    return new_stopwordless_text #возвращаем текст без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(write_path, 'r', encoding='utf-8') as f:\n",
    "        lemmatized_text = f.read().split() #открываем файл с лемматизированным текстом, считываем его в переменную, преобразуя в список\n",
    "        \n",
    "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().split('\\n') #открываем файл со стоп-словами, считываем его в переменную, преобразуя в список\n",
    "punctuation += \"« » — \\n ''\"\n",
    "new_stopwordless_text = delete_stopwords(lemmatized_text, stopwords) #записываем в переменную очищенный от стоп-слов текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = Counter(new_stopwordless_text) #создаем словарь, где ключом будет слово, а значением - его частотность\n",
    "total_words = (len(lemmatized_text)) #считаем абсолютную частотность\n",
    "for key, value in words_frequency.items(): #проходимся циклом по словарю и обновляем его значение на ipm\n",
    "    words_frequency[key] = value/total_words * 1000000 #обновление значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные слова:  [('пушкин', 18987.3417721519), ('толстой', 11251.758087201126), ('гоголь', 10900.14064697609), ('однажды', 10196.905766526019), ('лев', 8790.43600562588), ('любить', 7032.3488045007025), ('достоевский', 6680.731364275668), ('тургенев', 5274.261603375528), ('ребенок', 4922.644163150492), ('царствие', 4571.026722925457), ('окно', 4219.4092827004215), ('бульвар', 4219.4092827004215), ('идти', 4219.4092827004215), ('лермонтов', 4219.4092827004215), ('приходить', 3867.791842475387), ('тверской', 3867.791842475387), ('федор', 3867.791842475387), ('михайлович', 3867.791842475387), ('герцен', 3516.1744022503512), ('небесный', 3516.1744022503512)]\n"
     ]
    }
   ],
   "source": [
    "print('Самые частотные слова: ', words_frequency.most_common(20)) #получаем список 20 самых частотных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте полный морфологический разбор исходного текста. Напишите регулярное выражение, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и записывая их в новый словарь (форма -- ключ, часть речи -- значение). Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "f = open(read_path, 'r', encoding='utf-8')\n",
    "literary_anecdotes = f.read() #читаем файл с исходным текстом\n",
    "literary_anecdotes = ' '.join([w.strip(punctuation) for w in literary_anecdotes.split()]) #избавляемся от пунктуации\n",
    "word_analysis = m.analyze(literary_anecdotes) #проводим морфологический разбор текста и записываем результат в переменную\n",
    "\n",
    "form_pos = {} #создаем пустой словарь\n",
    "pos_reg = re.compile(\"[A-Z]+\") #создаем регулярное выражение для нахождени части речи (части речи отображаются в виде аббревиатур английскими буквами)\n",
    "for word in word_analysis: #проитерируемся по массиву с результатами анализа\n",
    "    try:\n",
    "        if len(word['analysis'])>0: #исключим варианты, где выдается пустой анализ\n",
    "            gr = word['analysis'][0]['gr'] #выделим грамматический разбор\n",
    "            pos = pos_reg.match(gr).group(0) #выделяем часть речи\n",
    "            form = word['text'] #выделим форму слова\n",
    "            if form != '': #исключим варианты, когда у нам появляется пустой символ\n",
    "                form_pos[form] = pos #записываем результаты в словарь, преобразовывая массив в строку\n",
    "    except (KeyError, IndexError): #обрабатываем исключения\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_frequency = Counter(form_pos.values()) #абсолютная частота частей речи\n",
    "relative_pos_frequency = dict() #создадим пустой словарь, куда запишем значения относительной частоты\n",
    "number_of_words_total = len(literary_anecdotes.split()) #найдем длину текста в словах\n",
    "for key, value in pos_frequency.items(): #пройдемся циклом по значениям абсолютной частоты частей речи\n",
    "    relative_pos_frequency[key] = value/number_of_words_total #получим относительную частоту частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Абсолютная частота частей речи: \n",
      " Counter({'S': 507, 'V': 426, 'A': 98, 'ADV': 89, 'SPRO': 54, 'APRO': 49, 'PR': 37, 'PART': 31, 'ADVPRO': 24, 'CONJ': 20, 'NUM': 9, 'INTJ': 8, 'ANUM': 3})\n",
      "Относительная частота частей речи: \n",
      " {'PR': 0.013009845288326301, 'S': 0.17827004219409281, 'V': 0.14978902953586498, 'A': 0.034458509142053444, 'ADV': 0.03129395218002813, 'SPRO': 0.0189873417721519, 'CONJ': 0.007032348804500703, 'PART': 0.01090014064697609, 'ADVPRO': 0.008438818565400843, 'APRO': 0.017229254571026722, 'ANUM': 0.0010548523206751054, 'INTJ': 0.0028129395218002813, 'NUM': 0.0031645569620253164}\n"
     ]
    }
   ],
   "source": [
    "print('Абсолютная частота частей речи: \\n', pos_frequency)\n",
    "print('Относительная частота частей речи: \\n', relative_pos_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если использовать ipm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_pos_frequency_ipm = dict() #создадим пустой словарь, куда запишем значения относительной частоты\n",
    "for key, value in pos_frequency.items(): #пройдемся циклом по значениям абсолютной частоты частей речи\n",
    "    relative_pos_frequency_ipm[key] = value/number_of_words_total * 1000000 #получим относительную частоту частей речи (используя ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная частота частей речи (используя ipm): \n",
      " {'PR': 13009.845288326302, 'S': 178270.04219409282, 'V': 149789.02953586497, 'A': 34458.509142053445, 'ADV': 31293.952180028133, 'SPRO': 18987.3417721519, 'CONJ': 7032.3488045007025, 'PART': 10900.14064697609, 'ADVPRO': 8438.818565400843, 'APRO': 17229.254571026722, 'ANUM': 1054.8523206751054, 'INTJ': 2812.9395218002815, 'NUM': 3164.5569620253164}\n"
     ]
    }
   ],
   "source": [
    "print('Относительная частота частей речи (используя ipm): \\n', relative_pos_frequency_ipm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
